{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `generate_causal_mask`\n",
    "Generates a causal mask to prevent attention to future tokens in sequence modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_causal_mask(seq_len, device):\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    causal_mask[causal_mask == 0] = -torch.inf\n",
    "    causal_mask[causal_mask == 1]=0\n",
    "    return causal_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scaled_dot_product`\n",
    "Performs the scaled dot-product attention calculation, applying the causal mask and returning the attention output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(self, q, k, v, causal_mask):\n",
    "    d_k = k.shape[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(torch.tensor(d_k))\n",
    "    if causal_mask is not None:\n",
    "        scaled += causal_mask\n",
    "    attention = torch.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    batch_size, _, max_sequence_length, _ = values.shape\n",
    "    values = values.permute(0, 2, 1, 3)\n",
    "    values = values.reshape(\n",
    "        batch_size, max_sequence_length, self.num_heads * self.d_head\n",
    "    )\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MultiHeadAttention`\n",
    "Implements multi-head attention mechanism by splitting input into multiple heads, performing scaled dot-product attention, and combining the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(self.d_model, 3 * self.d_model)\n",
    "        self.linear_layer = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x, causal_mask):\n",
    "        batch_size, max_sequence_length, _ = x.shape\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(\n",
    "            batch_size, max_sequence_length, self.num_heads, 3 * self.d_head\n",
    "        )\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        out = scaled_dot_product(self, q, k, v, causal_mask)\n",
    "        out = self.linear_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LayerNorm`\n",
    "Performs layer normalization to stabilize and accelerate training by normalizing inputs across the feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, 1, d_model))  # Correct shape\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, d_model))  # Correct shape\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(\n",
    "            dim=-1, unbiased=False, keepdim=True\n",
    "        )\n",
    "        normalized = (x - mean) / torch.sqrt(var + self.epsilon)\n",
    "        out = self.gamma * normalized + self.beta  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedForward`\n",
    "Defines a feed-forward neural network with two linear layers and a ReLU activation, used for position-wise transformation in the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, drop):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.Linear(d_ff, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MultiHeadCrossAttention`\n",
    "Implements multi-head cross-attention mechanism for attending to memory (encoder output) while processing input queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.qk_layer = nn.Linear(d_model, 2 * d_model)\n",
    "        self.linear_layer = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x,memory_from_encoder,causal_mask):\n",
    "        batch_size, max_sequence_length, _ = x.shape\n",
    "        qk = self.qk_layer(memory_from_encoder)\n",
    "        qk = qk.reshape(\n",
    "            batch_size, max_sequence_length, self.num_heads, 2 * self.d_head\n",
    "        )\n",
    "        x = x.reshape(batch_size, max_sequence_length, self.num_heads, 1 * self.d_head)\n",
    "        qk = qk.permute(0, 2, 1, 3)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        v = x\n",
    "        q, k = qk.chunk(2, dim=-1)\n",
    "        out = scaled_dot_product(self,q, k, v, causal_mask)\n",
    "        out = self.linear_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `EmbeddingWithPositionalEncoding`\n",
    "Provides word embeddings with added positional encodings to preserve the order of tokens in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_sequence_length, padding_idx):\n",
    "        super(EmbeddingWithPositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.positional_encoding = self._get_positional_encoding(max_sequence_length, d_model)\n",
    "\n",
    "    def _get_positional_encoding(self, max_sequence_length, d_model):\n",
    "        position = torch.arange(0, max_sequence_length).unsqueeze(1).float()\n",
    "        denominator = torch.pow(10000, torch.arange(0, d_model, 2).float() / d_model)\n",
    "        even_pe = torch.sin(position / denominator)\n",
    "        odd_pe = torch.cos(position / denominator)\n",
    "        pe = torch.cat([even_pe, odd_pe], dim=1)\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        embedded = self.embedding(input_sequence)\n",
    "        seq_len = input_sequence.size(1)\n",
    "        pe = self.positional_encoding[:, :seq_len, :].to(input_sequence.device)\n",
    "        embedded_with_pe = embedded + pe\n",
    "        return embedded_with_pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "The `Encoder` class implements a Transformer encoder block with multi-head attention, feed-forward layers, dropout for regularization, and layer normalization for stable training.\n",
    "\n",
    "Input -> Multi-Head Attention -> Dropout -> LayerNorm -> Residual Connection\n",
    "                           -> Feed-Forward -> Dropout -> Output\n",
    "\n",
    "<img src=\"Screenshot 2025-02-09 222302.png\" alt=\"Alt Text\" height=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, drop, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Assuming LayerNorm is a standard LayerNorm\n",
    "        self.dropout1 = nn.Dropout(p=drop)\n",
    "        self.ffn = FeedForward(d_model=d_model, d_ff=d_ff, drop=drop)\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # Assuming LayerNorm is a standard LayerNorm\n",
    "        self.dropout2 = nn.Dropout(p=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_dash = x\n",
    "        print(\"=\" * 50)\n",
    "        print(\"===== Encoder Forward Pass =====\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸ”¹ Input Shape: {x.shape}\")\n",
    "\n",
    "        # Attention block\n",
    "        print(\"ğŸ”¹ Processing Multi-Head Attention Layer\")\n",
    "        out = self.attention(x, causal_mask=None)\n",
    "        print(f\"ğŸ”¹ After Attention Layer: Input Shape: {x.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        # Dropout after attention\n",
    "        print(\"ğŸ”¹ Applying Dropout1\")\n",
    "        out = self.dropout1(out)\n",
    "        print(f\"ğŸ”¹ After Dropout1: Input Shape: {out.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        # Layer normalization and residual connection\n",
    "        print(\"ğŸ”¹ Applying LayerNorm1 with Residual Connection\")\n",
    "        out = self.norm1(out + x_dash)\n",
    "        print(f\"ğŸ”¹ After Norm1: Input Shape: {out.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        x_dash = out\n",
    "\n",
    "        # Feed-forward block\n",
    "        print(\"ğŸ”¹ Processing FeedForward Layer\")\n",
    "        out = self.ffn(out)\n",
    "        print(f\"ğŸ”¹ After FeedForward Layer: Input Shape: {x_dash.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        # Dropout after feed-forward\n",
    "        print(\"ğŸ”¹ Applying Dropout2\")\n",
    "        out = self.dropout2(out)\n",
    "        print(f\"ğŸ”¹ After Dropout2: Input Shape: {out.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "The `Decoder` class implements a Transformer decoder block with multi-head attention, cross-attention, feed-forward layers, dropout for regularization, and layer normalization for stable training.\n",
    "\n",
    "Input -> Multi-Head Attention -> Dropout -> LayerNorm -> Residual Connection\n",
    "                          -> Cross-Attention -> Dropout -> LayerNorm -> Residual Connection\n",
    "                          -> Feed-Forward -> Dropout -> LayerNorm -> Output\n",
    "\n",
    "<img src=\"Screenshot 2025-02-17 153855.png\" alt=\"Alt Text\" height=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, drop, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Assuming LayerNorm is a standard LayerNorm\n",
    "        self.dropout1 = nn.Dropout(p=drop)\n",
    "        self.cross_attention = MultiHeadCrossAttention(\n",
    "            d_model=d_model, num_heads=num_heads\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # Assuming LayerNorm is a standard LayerNorm\n",
    "        self.dropout2 = nn.Dropout(p=drop)\n",
    "        self.ffn = FeedForward(d_model=d_model, d_ff=d_ff, drop=drop)\n",
    "        self.norm3 = nn.LayerNorm(d_model)  # Assuming LayerNorm is a standard LayerNorm\n",
    "        self.dropout3 = nn.Dropout(p=drop)\n",
    "\n",
    "    def forward(self, x, memory_from_encoder, causal_mask):\n",
    "        x_dash = x\n",
    "        print(\"=\" * 50)\n",
    "        print(\"===== Decoder Forward Pass =====\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸ”¹ Input Shape: {x.shape}\")\n",
    "\n",
    "        # Attention block\n",
    "        print(\"ğŸ”¹ Processing Multi-Head Attention Layer\")\n",
    "        out = self.attention(x, causal_mask)\n",
    "        print(f\"ğŸ”¹ After Multi-Head Attention: Input Shape: {x.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        # Dropout after attention\n",
    "        print(\"ğŸ”¹ Applying Dropout1\")\n",
    "        out = self.dropout1(out)\n",
    "        print(f\"ğŸ”¹ After Dropout1: Input Shape: {out.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        # Layer normalization and residual connection\n",
    "        print(\"ğŸ”¹ Applying LayerNorm1 with Residual Connection\")\n",
    "        out = self.norm1(out + x_dash)\n",
    "        print(f\"ğŸ”¹ After Norm1: Input Shape: {out.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        x_dash = out\n",
    "\n",
    "        # Cross-attention block\n",
    "        print(\"ğŸ”¹ Processing Cross-Attention Layer\")\n",
    "        out = self.cross_attention(out, memory_from_encoder, causal_mask=None)\n",
    "        print(f\"ğŸ”¹ After Cross-Attention: Input Shape: {x_dash.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        # Dropout after cross-attention\n",
    "        print(\"ğŸ”¹ Applying Dropout2\")\n",
    "        out = self.dropout2(out)\n",
    "        print(f\"ğŸ”¹ After Dropout2: Input Shape: {out.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        # Layer normalization and residual connection\n",
    "        print(\"ğŸ”¹ Applying LayerNorm2 with Residual Connection\")\n",
    "        out = self.norm2(out + x_dash)\n",
    "        print(f\"ğŸ”¹ After Norm2: Input Shape: {out.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        x_dash = out\n",
    "\n",
    "        # Feed-forward block\n",
    "        print(\"ğŸ”¹ Processing FeedForward Layer\")\n",
    "        out = self.ffn(out)\n",
    "        print(f\"ğŸ”¹ After FeedForward: Input Shape: {x_dash.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        # Dropout after feed-forward\n",
    "        print(\"ğŸ”¹ Applying Dropout3\")\n",
    "        out = self.dropout3(out)\n",
    "        print(f\"ğŸ”¹ After Dropout3: Input Shape: {out.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        # Layer normalization and residual connection\n",
    "        print(\"ğŸ”¹ Applying LayerNorm3 with Residual Connection\")\n",
    "        out = self.norm3(out + x_dash)\n",
    "        print(f\"ğŸ”¹ After Norm3: Input Shape: {out.shape}, Output Shape: {out.shape}\")\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "The `Transformer` class integrates both the encoder and decoder sequences, where the encoder processes the input sequence, and the decoder generates the output sequence based on the encoder's memory. The final output is passed through a linear layer to map it to the desired output dimension.\n",
    "- **Input** -> Encoder Sequence -> Decoder Sequence -> Final Linear Layer -> **Output**\n",
    "\n",
    "### EncoderSequence\n",
    "The `EncoderSequence` class consists of multiple `Encoder` layers stacked sequentially. It processes an input through each layer and returns the final output after passing through all the layers.\n",
    "- **Input** -> Encoder Layer 1 -> Encoder Layer 2 -> ... -> Encoder Layer N -> **Output**\n",
    "\n",
    "### DecoderSequence\n",
    "The `DecoderSequence` class stacks multiple `Decoder` layers sequentially. It processes an input through each layer using causal masking and memory from the encoder, returning the final output after all layers.\n",
    "- **Input** -> Decoder Layer 1 -> Decoder Layer 2 -> ... -> Decoder Layer N -> **Output**\n",
    "\n",
    "<img src=\"transformer.jpg\" alt=\"Alt Text\" height=200/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSequence(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, drop, num_layers):\n",
    "        super(EncoderSequence, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                Encoder(d_model=d_model, num_heads=num_heads, d_ff=d_ff, drop=drop)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"ğŸ”¹ EncoderSequence input shape: {x.shape}\")\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            print(f\"ğŸ”¹ Passing through Encoder Layer {i + 1}\")\n",
    "            x = layer(x=x)\n",
    "        print(f\"ğŸ”¹ EncoderSequence output shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderSequence(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, drop, num_layers):\n",
    "        super(DecoderSequence, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                Decoder(d_model=d_model, num_heads=num_heads, d_ff=d_ff, drop=drop)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, causal_mask, memory_from_encoder):\n",
    "        print(f\"ğŸ”¹ DecoderSequence input shape: {x.shape}\")\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"ğŸ”¹ Passing through Decoder Layer {i + 1}\")\n",
    "            x = layer(x=x, causal_mask=causal_mask, memory_from_encoder=memory_from_encoder)\n",
    "        print(f\"ğŸ”¹ DecoderSequence output shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        output_dim,\n",
    "        drop=0.1,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = EncoderSequence(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            drop=drop,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.decoder = DecoderSequence(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            drop=drop,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.final_layer = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x, y, causal_mask):\n",
    "        encoder_output = self.encoder(x=x)\n",
    "        decoder_output = self.decoder(\n",
    "            x=y, causal_mask=causal_mask, memory_from_encoder=encoder_output\n",
    "        )\n",
    "        output = self.final_layer(decoder_output)\n",
    "        print(f\"ğŸ”¹ Shape of Final Layer: {output.shape}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Load the Data\n",
    "- Read English and Hindi sentences from text files.\n",
    "- Store them in lists for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets\\english.txt', 'r', encoding='utf-8') as file:\n",
    "    english_sentences = file.read()\n",
    "\n",
    "with open('datasets\\hindi.txt', 'r', encoding='utf-8') as file:\n",
    "    hindi_sentences = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization and Padding\n",
    "- Split each sentence into words.\n",
    "- Pad sentences with `<PAD>` to maintain a fixed length.\n",
    "- For decoder input, prepend `<START>` and append `<END>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = english_sentences.split(\"\\n\")\n",
    "hindi_sentences = hindi_sentences.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Make a good translation of the sentence that you are translating. Don't let translations into other languages influence you.\",\n",
       " 'à¤†à¤ª à¤œà¤¿à¤¸ à¤µà¤¾à¤•à¥à¤¯ à¤•à¤¾ à¤…à¤¨à¥à¤µà¤¾à¤¦ à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚, à¤‰à¤¸ à¤¹à¥€ à¤•à¤¾ à¤…à¤šà¥à¤›à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤…à¤¨à¥à¤µà¤¾à¤¦ à¤•à¤°à¥‡à¤‚à¥¤ à¤¦à¥‚à¤¸à¤°à¥€ à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤•à¥‡ à¤…à¤¨à¥à¤µà¤¾à¤¦à¥‹à¤‚ à¤¸à¥‡ à¤ªà¥à¤°à¤­à¤¾à¤µà¤¿à¤¤ à¤¨ à¤¹à¥‹à¤¨à¥‡ à¤¦à¥‡à¤‚à¥¤')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[-1],hindi_sentences[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Help!',\n",
       " 'Jump.',\n",
       " 'Jump.',\n",
       " 'Jump.',\n",
       " 'Hello!',\n",
       " 'Hello!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Got it?',\n",
       " \"I'm OK.\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤¬à¤šà¤¾à¤“!',\n",
       " 'à¤‰à¤›à¤²à¥‹.',\n",
       " 'à¤•à¥‚à¤¦à¥‹.',\n",
       " 'à¤›à¤²à¤¾à¤‚à¤—.',\n",
       " 'à¤¨à¤®à¤¸à¥à¤¤à¥‡à¥¤',\n",
       " 'à¤¨à¤®à¤¸à¥à¤•à¤¾à¤°à¥¤',\n",
       " 'à¤µà¤¾à¤¹-à¤µà¤¾à¤¹!',\n",
       " 'à¤šà¤¿à¤¯à¤°à¥à¤¸!',\n",
       " 'à¤¸à¤®à¤à¥‡ à¤•à¤¿ à¤¨à¤¹à¥€à¤‚?',\n",
       " 'à¤®à¥ˆà¤‚ à¤ à¥€à¤• à¤¹à¥‚à¤à¥¤']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2869, 2869)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences),len(hindi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = \"<START>\"\n",
    "EOS = \"<END>\"\n",
    "PAD = \"<PAD>\"\n",
    "max_sequence_length=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_encoder_input(sentence, max_sequence_length):\n",
    "    tokenized_sentence = sentence.split()\n",
    "    if len(tokenized_sentence) > max_sequence_length:\n",
    "        return None\n",
    "    padding_length = max_sequence_length - len(tokenized_sentence)\n",
    "    tokenized_sentence += [PAD] * padding_length  \n",
    "    return tokenized_sentence\n",
    "\n",
    "def prepare_decoder_input(sentence, max_sequence_length):\n",
    "    tokenized_sentence = sentence.split()\n",
    "    if len(tokenized_sentence) > max_sequence_length - 2:\n",
    "        return None\n",
    "    tokenized_sentence = [SOS] + tokenized_sentence + [EOS]\n",
    "    padding_length = max_sequence_length - len(tokenized_sentence)\n",
    "    tokenized_sentence += [PAD] * padding_length \n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Sentence Processing\n",
    "- Iterate through sentence pairs.\n",
    "- Convert them into tokenized and padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sequence=[]\n",
    "hindi_sequence =[]\n",
    "for index in range(len(english_sentences)):\n",
    "    english_sentence, hindi_sentence = english_sentences[index], hindi_sentences[index]\n",
    "    encoder_input = prepare_encoder_input(english_sentence, max_sequence_length)\n",
    "    decoder_input = prepare_decoder_input(hindi_sentence, max_sequence_length)\n",
    "    if encoder_input is not None and decoder_input is not None:\n",
    "        english_sequence.append(encoder_input)\n",
    "        hindi_sequence.append(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed English Sequence: [['Help!' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      "  '<PAD>']]\n",
      "Processed Hindi Sequence: [['<START>' 'à¤¬à¤šà¤¾à¤“!' '<END>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      "  '<PAD>' '<PAD>']]\n"
     ]
    }
   ],
   "source": [
    "english_sequence=np.array(english_sequence)\n",
    "hindi_sequence=np.array(hindi_sequence)\n",
    "\n",
    "print(\"Processed English Sequence:\", english_sequence[:1])\n",
    "print(\"Processed Hindi Sequence:\", hindi_sequence[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2249, 2249)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sequence),len(hindi_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Encoding Words to Indices\n",
    "- Use `LabelEncoder` to convert words into numerical indices.\n",
    "- This transforms text into numerical format for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_label=LabelEncoder()\n",
    "hi_label=LabelEncoder()\n",
    "\n",
    "english_words = english_sequence.flatten()\n",
    "hindi_words = hindi_sequence.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word_indices=eng_label.fit_transform(english_words)\n",
    "hi_word_indices=hi_label.fit_transform(hindi_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Assign Special Token Indices\n",
    "- Identify indices for `<PAD>`, `<START>`, and `<END>`.\n",
    "- These indices help in masking and sequence generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English PAD Index: 23\n",
      "Hindi PAD Index: 8\n",
      "START (SOS) Index: 9\n",
      "END (EOS) Index: 7\n",
      "Encoder Vocabulary Size: 2767\n",
      "Decoder Vocabulary Size: 2530\n"
     ]
    }
   ],
   "source": [
    "eng_pad_index = np.where(eng_label.classes_ == \"<PAD>\")[0][0]\n",
    "hi_pad_index = np.where(hi_label.classes_ == \"<PAD>\")[0][0]\n",
    "sos_index = np.where(hi_label.classes_ == \"<START>\")[0][0]\n",
    "eos_index = np.where(hi_label.classes_ == \"<END>\")[0][0]\n",
    "\n",
    "print(f\"English PAD Index: {eng_pad_index}\")\n",
    "print(f\"Hindi PAD Index: {hi_pad_index}\")\n",
    "print(f\"START (SOS) Index: {sos_index}\")\n",
    "print(f\"END (EOS) Index: {eos_index}\")\n",
    "\n",
    "encoder_vocab_size = len(eng_label.classes_)\n",
    "decoder_vocab_size = len(hi_label.classes_)\n",
    "\n",
    "print(f\"Encoder Vocabulary Size: {encoder_vocab_size}\")\n",
    "print(f\"Decoder Vocabulary Size: {decoder_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Splitting\n",
    "- Reshape sequences into the required input format.\n",
    "- Split the dataset into training and testing sets (80-20 split).\n",
    "\n",
    "## Final Output\n",
    "- `X_train`, `X_test`: Encoded English sentence inputs.\n",
    "- `y_train`, `y_test`: Encoded Hindi sentence outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = eng_word_indices.reshape(len(english_sequence), max_sequence_length)\n",
    "y = hi_word_indices.reshape(len(hindi_sequence), max_sequence_length)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation and Training Setup\n",
    "\n",
    "## Step 1: Create a Custom Dataset\n",
    "- Convert `X_train`, `y_train`, `X_test`, and `y_test` into PyTorch tensors.\n",
    "- Define a dataset class to handle indexing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        self.X=torch.tensor(X)\n",
    "        self.y=torch.tensor(y)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index],self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Model Parameters\n",
    "- `d_model`: Dimensionality of embeddings.\n",
    "- `num_heads`: Number of attention heads.\n",
    "- `d_head`: Size of each attention head.\n",
    "- `d_ff`: Feedforward network dimension.\n",
    "- `batch_size`: Number of samples per batch.\n",
    "- `num_layers`: Number of Transformer layers.\n",
    "- `num_epochs`: Total number of training epochs.\n",
    "- `output_dim`: Vocabulary size of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_head = 64\n",
    "d_ff = 2048\n",
    "batch_size = 32\n",
    "num_layers = 6\n",
    "num_epochs=10\n",
    "output_dim = decoder_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Data Loaders\n",
    "- Use `DataLoader` to batch and shuffle training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Embedding Layers\n",
    "- Create `EmbeddingWithPositionalEncoding` for both encoder and decoder.\n",
    "- Use vocabulary sizes and padding indices for proper embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_embedding = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=encoder_vocab_size, d_model=d_model, max_sequence_length=max_sequence_length, padding_idx=eng_pad_index\n",
    ")\n",
    "\n",
    "decoder_embedding = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=decoder_vocab_size, d_model=d_model, max_sequence_length=max_sequence_length, padding_idx=hi_pad_index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Transformer Model\n",
    "- Define the Transformer model with the specified architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=Transformer(d_model,num_heads,d_ff,output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define Loss Function and Optimizer\n",
    "- Use `CrossEntropyLoss` with `ignore_index=0` to handle padding.\n",
    "- Use Adam optimizer with a learning rate of `0.0001` for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation of Transformer Model\n",
    "\n",
    "## Step 1: Initialize Training Loss Storage\n",
    "- Maintain a list `train_losses` to store the average loss for each epoch.\n",
    "\n",
    "## Step 2: Training Loop\n",
    "- Iterate through the number of epochs.\n",
    "- Set the Transformer model to training mode.\n",
    "- Loop through batches in the `train_loader`:\n",
    "  - Pass the input through the embedding layers.\n",
    "  - Generate a causal mask for the decoder sequence.\n",
    "  - Compute the model output using the Transformer.\n",
    "  - Shift the target sequence for teacher forcing.\n",
    "  - Compute the cross-entropy loss.\n",
    "  - Perform backpropagation and update weights.\n",
    "  - Accumulate loss to compute the average epoch loss.\n",
    "- Store and print the average loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 1: 5.0489\n",
      "==================================================\n",
      "Epoch 2/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 2: 3.9583\n",
      "==================================================\n",
      "Epoch 3/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 3: 3.3528\n",
      "==================================================\n",
      "Epoch 4/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 4: 2.6206\n",
      "==================================================\n",
      "Epoch 5/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 5: 2.0891\n",
      "==================================================\n",
      "Epoch 6/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 6: 1.7088\n",
      "==================================================\n",
      "Epoch 7/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 7: 1.4033\n",
      "==================================================\n",
      "Epoch 8/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 8: 1.1806\n",
      "==================================================\n",
      "Epoch 9/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 9: 0.9921\n",
      "==================================================\n",
      "Epoch 10/10\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([7, 10, 512]), Output Shape: torch.Size([7, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([7, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([7, 10, 2530])\n",
      "ğŸ”¹ Average Training Loss for Epoch 10: 0.8306\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    transformer.train()\n",
    "    for batch_idx, (encoder_sequence, decoder_sequence) in enumerate(train_loader):\n",
    "        x = encoder_embedding(encoder_sequence)\n",
    "        y = decoder_embedding(decoder_sequence)\n",
    "        causal_mask = generate_causal_mask(max_sequence_length, y.device)\n",
    "        out = transformer(x=x, y=y, causal_mask=causal_mask)\n",
    "        y_input = decoder_sequence[:, :-1]\n",
    "        y_target = decoder_sequence[:, 1:]\n",
    "        out_flat = out[:, :-1, :].contiguous().view(-1, decoder_vocab_size)\n",
    "        y_target_flat = y_target.contiguous().view(-1)\n",
    "        loss = criterion(out_flat, y_target_flat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    print(f\"ğŸ”¹ Average Training Loss for Epoch {epoch + 1}: {avg_epoch_loss:.4f}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation Loop\n",
    "- Set the model to evaluation mode (disables dropout and batch norm).\n",
    "- Disable gradient computation for inference.\n",
    "- Loop through the test dataset:\n",
    "  - Pass the input through the embedding layers.\n",
    "  - Generate a causal mask for decoding.\n",
    "  - Compute the model output using the Transformer.\n",
    "  - Prepare target sequences for loss computation.\n",
    "  - Compute the test loss without updating weights.\n",
    "  - Accumulate and store the average test loss.\n",
    "- Print the final test loss after evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([32, 10, 512]), Output Shape: torch.Size([32, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([32, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([32, 10, 2530])\n",
      "ğŸ”¹ EncoderSequence input shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Passing through Encoder Layer 1\n",
      "==================================================\n",
      "===== Encoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Attention Layer: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward Layer: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ EncoderSequence output shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ DecoderSequence input shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Passing through Decoder Layer 1\n",
      "==================================================\n",
      "===== Decoder Forward Pass =====\n",
      "==================================================\n",
      "ğŸ”¹ Input Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Processing Multi-Head Attention Layer\n",
      "ğŸ”¹ After Multi-Head Attention: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Applying Dropout1\n",
      "ğŸ”¹ After Dropout1: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm1 with Residual Connection\n",
      "ğŸ”¹ After Norm1: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Processing Cross-Attention Layer\n",
      "ğŸ”¹ After Cross-Attention: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Applying Dropout2\n",
      "ğŸ”¹ After Dropout2: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm2 with Residual Connection\n",
      "ğŸ”¹ After Norm2: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Processing FeedForward Layer\n",
      "ğŸ”¹ After FeedForward: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Applying Dropout3\n",
      "ğŸ”¹ After Dropout3: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Applying LayerNorm3 with Residual Connection\n",
      "ğŸ”¹ After Norm3: Input Shape: torch.Size([2, 10, 512]), Output Shape: torch.Size([2, 10, 512])\n",
      "==================================================\n",
      "ğŸ”¹ DecoderSequence output shape: torch.Size([2, 10, 512])\n",
      "ğŸ”¹ Shape of Final Layer: torch.Size([2, 10, 2530])\n",
      "ğŸ”¹ Average Test Loss: 1.4231\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "transformer.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, (encoder_sequence, decoder_sequence) in enumerate(test_loader):\n",
    "        x = encoder_embedding(encoder_sequence)\n",
    "        y = decoder_embedding(decoder_sequence)\n",
    "        causal_mask = generate_causal_mask(max_sequence_length, y.device)\n",
    "        out = transformer(x=x, y=y, causal_mask=causal_mask)\n",
    "        y_input = decoder_sequence[:, :-1]\n",
    "        y_target = decoder_sequence[:, 1:]\n",
    "        out_flat = out[:, :-1, :].contiguous().view(-1, decoder_vocab_size)\n",
    "        y_target_flat = y_target.contiguous().view(-1)\n",
    "        loss = criterion(out_flat, y_target_flat)\n",
    "        test_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_test_loss = test_loss / num_batches if num_batches > 0 else 0\n",
    "    test_losses.append(avg_test_loss)\n",
    "    print(f\"ğŸ”¹ Average Test Loss: {avg_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "Transformer                                        --\n",
      "â”œâ”€EncoderSequence: 1-1                             --\n",
      "â”‚    â””â”€ModuleList: 2-1                             --\n",
      "â”‚    â”‚    â””â”€Encoder: 3-1                           7,348,736\n",
      "â”œâ”€DecoderSequence: 1-2                             --\n",
      "â”‚    â””â”€ModuleList: 2-2                             --\n",
      "â”‚    â”‚    â””â”€Decoder: 3-2                           8,137,728\n",
      "â”œâ”€Linear: 1-3                                      1,297,890\n",
      "===========================================================================\n",
      "Total params: 16,784,354\n",
      "Trainable params: 16,784,354\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "Transformer                                        --\n",
       "â”œâ”€EncoderSequence: 1-1                             --\n",
       "â”‚    â””â”€ModuleList: 2-1                             --\n",
       "â”‚    â”‚    â””â”€Encoder: 3-1                           7,348,736\n",
       "â”œâ”€DecoderSequence: 1-2                             --\n",
       "â”‚    â””â”€ModuleList: 2-2                             --\n",
       "â”‚    â”‚    â””â”€Decoder: 3-2                           8,137,728\n",
       "â”œâ”€Linear: 1-3                                      1,297,890\n",
       "===========================================================================\n",
       "Total params: 16,784,354\n",
       "Trainable params: 16,784,354\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(transformer,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
